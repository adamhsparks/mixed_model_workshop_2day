---
title: "Mixed Models in R"
subtitle: "A Practical Introduction"
author: "Henrik Singmann (University of Warwick)<br/>Twitter: <a href='https://twitter.com/HenrikSingmann'>@HenrikSingmann</a>"
date: "January 2019"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "my-theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
---



```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# see: https://github.com/yihui/xaringan
# install.packages("xaringan")
# see: 
# https://github.com/yihui/xaringan/wiki
# https://github.com/gnab/remark/wiki/Markdown
options(width=110)
options(digits = 4)
```

# Mixed Model Analysis

Steps for running a mixed model analysis:
1. Identify desired fixed-effects structure
2. Identify random-effects grouping factors
3. Identify *maximal random-effects structure justified by the design*:
    - Which factors/terms vary within levels of (i.e. are crossed with) each random-effects grouping factor? 
    - Are there replicates within factor levels (or parameters/coefficients) for levels of random-effects grouping factor?
4. Choose method for calculating *p*-values and fit maximal model
5. In case of singular fits: Iteratively reduce random-effects structure until all degenerate random-effects parameters are removed/fit is not singular.


---

# Mixed Models

- Modern class of statistical models that extend regular regression models
- Implement *partial pooling* via random-effects parameters
- Random-effects parameters can account for dependencies among related data points

--

.pull-left[

### Fixed Effects
- Overall  or *population-level average* effect of specific model term (i.e., main effect, interaction, parameter) on dependent variable
- Independent of stochastic variability controlled for by random effects
- Hypothesis tests on fixed effect interpreted as hypothesis tests for terms in standard ANOVA or regression model
- Possible to test specific hypotheses among factor levels (e.g., planned contrasts)

]

.pull-right[
### Random Effects

- *Random-effects grouping factors*: Categorical variables that capture random or stochastic variability (e.g., participants, items, groups, or other hierarchical-structures).
- In experimental settings, random effects grouping factors often part of design one wants to generalize over.
- Random effects factor out idiosyncrasies of sample, thereby providing a more general estimate of the fixed effects of interest.
- *Random-effects parameters*: Provide each level of random-effects grouping factor with idiosyncratic parameter set.

]

---
class: inline-grey, small

### Mixed Models Formulas with `lme4`/`afex` 

- Full mixed model is specified in one formula. For example: `dv ~ fixed + (random|id)`
- Random effects are inside parentheses with a `|`:
  - random-effects parameters left of `|`
  - random-effects grouping factor is right of `|`
- Fixed effects are usually outside parentheses

| Formula | Interpretation  |
| ------------------------|----------------------------------|
| <code>(1&#124;s)</code>               | random intercepts for `s` (i.e., by-`s` random intercepts) | 
| <code>(1&#124;s)+(1&#124;i)</code>   |  by-`s` and by-`i` (i.e., crossed) random intercepts |
| <code>(a&#124;s)</code> or <code>(1+a&#124;s)</code>               | by-`s` random intercepts and by-`s` random slopes for `a` plus their correlation|
| <code>(a*b&#124;s)</code>                 | by-`s` random intercepts and by-`s` random slopes for `a`, `b`, and the `a:b` interaction plus correlations among the by-`s` random effects parameters |
| <code>(0+a&#124;s)</code>  | by-`s` random slopes for `a` and no random intercept |
| <code>(a&#124;&#124;s)</code> | by-`s` random intercepts and by-`s` random slopes for `a`, but no correlation (expands to: <code>(0+a&#124;s) + (1&#124;s)</code>) |

*Note*: Suppressing correlation parameters via `||` works only for numerical variables in `lmer` and not for factors. In `afex::mixed` it works if  `expand_re = TRUE`.


---
class: small

# Example Data

![](cognition_cutout.png)


---
class: small, inline-grey

### Skovgaard-Olsen et al. (2016)

- Conditional = *if-then* statement; e.g., If global warning continues, London will be flooded.
- Bayesian reasoning often assumes 'the Equation': *P*(if *A* then *B*) = *P*(*B*|*A*)
- Our question: Does 'the Equation' hold? 
- Participants provide idiosyncratic estimates of both *P*(if *A* then *B*) and *P*(*B*|*A*).

- ~100 participants recruited via `crowdflower.com` worked on 12 items:

> Sophia's scenario: Sophia wishes to find a nice present for her 13-year-old son, Tim, for Christmas. She is running on a tight budget, but she knows that Tim loves participating in live role-playing in the forest and she is really skilled at sewing the orc costumes he needs. Unfortunately, she will not be able to afford the leather parts that such costumes usually have, but she will still be able to make them look nice.

--

(1) *P*(*B*|*A*) (`B_given_A`):
  > Suppose Sophia makes Tim an orc costume.
  > Under this assumption, how probable is it that the following sentence is true:   
  > Tim will be excited about his present.

--

(2) *P*(if *A* then *B*) (`if_A_then_B`):
  > Could you please rate the probability that the following sentence is true:   
  > IF Sophia makes Tim an orc costume, THEN he will be excited about his present.

---

class: small 

.pull-left[
### Skovgaard-Olsen et al. (2016)

- Does 'the Equation' (i.e., *P*(if *A* then *B*) = *P*(*B*|*A*)) hold? 

- Participants provided idiosyncratic estimates of *P*(if *A* then *B*) (`if_A_then_B`) and *P*(*B*|*A*) (`B_given_A`) for each item.
- Each participant worked on 12 items.


```{r, message=FALSE}
# Session -> Set Working Directory ->
# -> To Source File Location
load("ssk16_dat_prepared_ex2.rda") 
# full data: https://osf.io/j4swp/
str(dat2, width=50, strict.width = "cut")
```

]

.pull-right[

```{r, fig.height=6, dev='svg', message=FALSE}
library("tidyverse")
theme_set(theme_bw(base_size = 17))
ggplot(data = dat2) + 
  geom_point(mapping = aes(x = B_given_A, 
                           y = if_A_then_B), 
             alpha = 0.2, pch = 16, size = 3) + 
  coord_fixed()
```

]


---
class:small


# Example Data: Skovgaard-Olsen et al. (2016)

- Is there an overall association between *P*(if *A* then *B*) and *P*(*B*|*A*)?
- For technical reason, we work with scaled variables (i.e., $(x-50)/100$). `lme4` works better with variables near -1 to 1.

.pull-left[

```{r}
m_fixed <- lm(if_A_then_B_c ~ B_given_A_c, dat2)
summary(m_fixed)
```
]
.pull-right[

```{r, echo=FALSE, dpi=300, fig.width=4, fig.height=4, dev='svg'}
ggplot(data = dat2, aes(x = B_given_A_c, y = if_A_then_B_c)) + 
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(intercept = coef(m_fixed)[1], slope = coef(m_fixed)[2], size = 1.5) +
  coord_fixed() 
```

$$y=\beta_0 + \beta_{B|A}X_{B|A} + \epsilon$$

]

---
class: small

.pull-left2[
### Fixed Effects Model

$$y=\beta_0 + \beta_{B|A}X_{B|A} + \epsilon$$

- $\beta$ are scalar regression parameters
- $X$ is a covariate vector (numerical independent variable)
- assumes error vector $\epsilon$ is *iid*, $\epsilon \sim \mathcal{N}(0, \sigma^2_{\epsilon})$, which is likely false.
]

.pull-right2[
```{r, echo=FALSE, fig.width=3.5, fig.height=4, dev='svg'}
ggplot(data = dat2, aes(x = B_given_A_c, y = if_A_then_B_c)) + 
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(intercept = coef(m_fixed)[1], slope = coef(m_fixed)[2], size = 1.5) +
  coord_fixed()
```
]

--

### A First Mixed Effects Model

$$y=\beta_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + \epsilon$$
- $S_{B|A}$ is a zero-centered vector of participant specific random effects: $S_{B|A} \sim \mathcal{N}(0, \sigma^2_{S_{B|A}})$
- $S_{B|A}$ provides each participant with their own regression weight: overall $\beta_{B|A}$ plus idiosyncratic $S_{B|A}$.
- As parameter, model estimates variance of random effects vector, $\sigma^2_{S_{B|A}}$.
- As $S_{B|A}$ alters the regression slope $\beta_{B|A}$, also known as *random slope*.
- In `lme4::lmer` syntax: `lmer(if_A_then_B ~ B_given_A + (0+B_given_A|p_id), dat2)`

---
class: small


.pull-left2[
### Fixed Effects Model

$$y=\beta_0 + \beta_{B|A}X_{B|A} + \epsilon$$

- $\beta$ are scalar regression parameters
- $X$ is a covariate vector (numerical independent variable)
- assumes error vector $\epsilon$ is *iid*, $\epsilon \sim \mathcal{N}(0, \sigma^2_{\epsilon})$, which is likely false.
]

.pull-right2[
```{r, echo=FALSE, dpi=500, fig.width=3.5, fig.height=4, warning=FALSE, message=FALSE}
library("lme4")
m_tmp <- lmer(if_A_then_B_c ~ B_given_A_c + (0+B_given_A_c|p_id), dat2)
rnd_coefs <- as_tibble(coef(m_tmp)$p_id)

ggplot(data = dat2, aes(x = B_given_A_c, y = if_A_then_B_c)) + 
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs, aes_string(intercept = "`(Intercept)`", 
                                     slope = "B_given_A_c"), 
              color = "lightgrey", size = 1.2) +
  geom_abline(intercept = coef(m_fixed)[1], slope = coef(m_fixed)[2], size = 1.5) +
  coord_fixed()
```
]

### A First Mixed Effects Model

$$y=\beta_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + \epsilon$$
- $S_{B|A}$ is a zero-centered vector of participant specific random effects: $S_{B|A} \sim \mathcal{N}(0, \sigma^2_{S_{B|A}})$
- $S_{B|A}$ provides each participant with their own regression weight: overall $\beta_{B|A}$ plus idiosyncratic $S_{B|A}$.
- As parameter, model estimates variance of random effects vector, $\sigma^2_{S_{B|A}}$.
- As $S_{B|A}$ alters the regression slope $\beta_{B|A}$, also known as *random slope*.
- In `lme4::lmer` syntax: `lmer(if_A_then_B ~ B_given_A + (0+B_given_A|p_id), dat2)`


---
class: small

### A More Reasonable Mixed Model

First model did not allow for different intercepts, $\beta_0$, for each participant. 

$$y=\beta_0 + S_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + \epsilon$$
- Model now has a random intercept, $S_0$, and a random slope, $S_{B|A}$.
- $S$-vectors still zero-centered vectors of participant specific random effects. However, we now estimate both, variance and covariance of random effects:
  
$$\left( \begin{matrix} S_0 \\ S_{B|A} \end{matrix} \right)
 \sim \mathcal{N}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{S_0}&\rho_{S_{0},S_{B|A}}\sigma_{S_{0}}\sigma_{S_{B|A}} \\ \rho_{S_{B|A},S_{0}}\sigma_{S_{0}}\sigma_{S_{B|A}}&\sigma^2_{S_{B|A}} \end{matrix} \right] \right)$$

Each $S_i$ idiosyncratic intercept & slope: `lmer(if_A_then_B ~ B_given_A + (B_given_A|p_id), dat2)`

```{r, echo=FALSE, dpi=300, fig.width=3.5, fig.height=3.5, warning=FALSE, out.width='20%'}
m_tmp <- lmer(if_A_then_B_c ~ B_given_A_c + (1+B_given_A_c|p_id), dat2)
rnd_coefs <- as_tibble(coef(m_tmp)$p_id)

ggplot(data = dat2, aes(x = B_given_A_c, y = if_A_then_B_c)) + 
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs, aes_string(intercept = "`(Intercept)`", 
                                     slope = "B_given_A_c"), 
              color = "lightgrey", size = 1.2) +
  geom_abline(intercept = coef(m_fixed)[1], slope = coef(m_fixed)[2], size = 1.5) +
  coord_fixed() 
```

---

### Interim Summary I

*Fixed-effects parameters*: Overall effect of specific model term on dependent variable

*Random-effects parameters*: 
- zero-centered offsets/displacements for each level of random-effects grouping factor
- added to specific fixed-effects parameter
- assumed to follow normal distribution which provides hierarchical shrinkage, thereby avoiding over-fitting
- should be added to each parameter that varies within the levels of a random-effects grouping factor (i.e., factor is *crossed* with random-effects grouping factor)

```{r, echo=FALSE, dpi=300, fig.width=3.5, fig.height=4, warning=FALSE , out.width='25%'}
rnd_coefs <- as_tibble(coef(m_tmp)$p_id)
ggplot(data = dat2, aes(x = B_given_A_c, y = if_A_then_B_c)) + 
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs, aes_string(intercept = "`(Intercept)`", 
                                     slope = "B_given_A_c"), 
              color = "lightgrey", size = 1.2) +
  geom_abline(intercept = coef(m_fixed)[1], slope = coef(m_fixed)[2], size = 1.5) +
  coord_fixed()
```

---
class: small

```{r}
library("lme4")
m_r <- lmer(if_A_then_B_c ~ B_given_A_c + (1+B_given_A_c|p_id), dat2)
summary(m_r)
```

----

# Exercise 2: Fitting First Mixed Model

- Open `exercises/exercise_2_FIRST.Rmd`.

- **Task 1:** Goal is to recreate the first two models from the slides, add one more model, and inspect the results.

- Dependent variable: `if_A_then_B_c`
- Fixed effect: `B_given_A_c`

1. The first model only has by-participant (i.e., by `p_id`) random-slopes for `B_given_A_c`.
2. The second model has has by-participant random-slopes for `B_given_A_c` and random-intercepts, as well as the correlation between the random-effects parameters.
3. The third (and new) model only has by-participant random intercepts and no random slopes.


- Which model shows a singular fit or other convergence problems? Do you have an idea where the convergence problems come from?
- What do the parameter estimates of the random effects mean?
- Does the choice of random-effects structure affect the fixed-effect estimates and/or inferential statistics?
- Which model do you think would be the best to report?


- **Task 2**: Suppressing the Correlation. The only model we have not yet considered for this data is a model without correlation among the random-effects parameters. Can you set it up?




---
class: small
### Example Data Extended: Skovgaard-Olsen et al. (2016)

- Conditional = *if-then* statement; e.g., If global warning continues, London will be flooded.
- Bayesian reasoning often assumes 'the Equation': *P*(if *A* then *B*) = *P*(*B*|*A*)
- Our question: Does the Equation hold even if no apparent relationship between *A* and *B*? 
  - positive relevance (PO): *A* is a reason for *B* 
  - irrelevance (IR): *A* and *B* have no apparent relationship 


--


> Sophia's scenario: Sophia wishes to find a nice present for her 13-year-old son, Tim, for Christmas. She is running on a tight budget, but she knows that Tim loves participating in live role-playing in the forest and she is really skilled at sewing the orc costumes he needs. Unfortunately, she will not be able to afford the leather parts that such costumes usually have, but she will still be able to make them look nice.

--

### Positive relevance (PO)

> IF Sophia makes Tim an orc costume, THEN he will be excited about his present.

--

### Irrelevance (IR)

> IF Sophia regularly wears shoes, THEN Tim will be excited about his present.

---

# Example Data: Skovgaard-Olsen et al. (2016)

```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=4, warning=FALSE, out.width='80%'}
ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) +
  facet_wrap(~ rel_cond) + 
  coord_fixed()
```

---
class: small

.pull-left[

### Fixed Effects Model

```{r}
afex::set_sum_contrasts()
m_fixed <- lm(if_A_then_B_c ~ 
                B_given_A_c*rel_cond, dat2)
summary(m_fixed)
```
]
.pull-right[

```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=4, warning=FALSE}
slopes <- data_frame(
  intercept = coef(m_fixed)[1] + c(1, -1)*coef(m_fixed)[3],
  slope = coef(m_fixed)[2] + c(1, -1)*coef(m_fixed)[4],
  rel_cond = factor(c("PO", "IR"), levels = c("PO", "IR"))
)
ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = slopes, aes(intercept = intercept, slope = slope),
              size = 1.5) +
  facet_wrap(~ rel_cond) + 
  coord_fixed()
```

$$y=\beta_0 + \beta_{B|A}X_{B|A} + \beta_{r}X_{r} + \beta_{I}X_{I} + \epsilon$$

]

---
class: small

.pull-left[
### Fixed Effects Model

$$y=\beta_0 + \beta_{B|A}X_{B|A} + \beta_{r}X_{r} + \beta_{I}X_{I} + \epsilon$$

- $\beta$ are scalar regression parameters
- $X$ are covariate vectors (numerical independent variables with -1 and 1 for factors)
- assumes error vector $\epsilon$ is *iid*, $\epsilon \sim \mathcal{N}(0, \sigma^2_{\epsilon})$, which is likely false.
]

.pull-right[
```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=4}
ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = slopes, aes(intercept = intercept, slope = slope), 
              size = 1.5) +
  facet_wrap(~ rel_cond) + 
  coord_fixed()
```
]

--

### A First Mixed Effects Model

$$y=\beta_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + \beta_{r}X_{r} + \beta_{I}X_{I} + \epsilon$$
- $S_{B|A}$ is a zero-centered vector of participant specific random effects: $S_{B|A} \sim \mathcal{N}(0, \sigma^2_{S_{B|A}})$
- $S_{B|A}$ provides each participant with their own regression weight: overall $\beta_{B|A}$ plus idiosyncratic $S_{B|A}$.
- As parameter, model estimates variance of random effects vector, $\sigma^2_{S_{B|A}}$.
- As $S_{B|A}$ alters the regression slope $\beta_{B|A}$, also known as *random slope*.
- In `lme4::lmer` syntax: `lmer(if_A_then_B ~ B_given_A*rel_cond + (0+B_given_A|p_id), dat2)`

---
class: small


.pull-left[
### Fixed Effects Model

$$y=\beta_0 + \beta_{B|A}X_{B|A} + \beta_{r}X_{r} + \beta_{I}X_{I} + \epsilon$$

- $\beta$ are scalar regression parameters
- $X$ are covariate vectors (numerical independent variables with -1 and 1 for factors)
- assumes error vector $\epsilon$ is *iid*, $\epsilon \sim \mathcal{N}(0, \sigma^2_{\epsilon})$, which is likely false.
]

.pull-right[
```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=4, warning=FALSE}
m_tmp <- lmer(if_A_then_B_c ~ B_given_A_c*rel_cond + (0+B_given_A_c|p_id), dat2)
rnd_coefs <- as_tibble(coef(m_tmp)$p_id)

rnd_coefs2 <- bind_rows(
  tibble(
    rel_cond = factor("PO", levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` + rnd_coefs$rel_cond1,
    slope = rnd_coefs$B_given_A_c + rnd_coefs$`B_given_A_c:rel_cond1`
  ),
  tibble(
    rel_cond = factor(c("IR"), levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` - rnd_coefs$rel_cond1,
    slope = rnd_coefs$B_given_A_c - rnd_coefs$`B_given_A_c:rel_cond1`
  )
)

ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs2, aes(intercept = intercept, slope = slope), 
              size = 1.2, color = "lightgrey") +
  geom_abline(data = slopes, aes(intercept = intercept, slope = slope), 
              size = 1.5) +
  facet_wrap(~ rel_cond) + 
  coord_fixed()
```
]

### A First Mixed Effects Model

$$y=\beta_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + \beta_{r}X_{r} + \beta_{I}X_{I} + \epsilon$$
- $S_{B|A}$ is a zero-centered vector of participant specific random effects: $S_{B|A} \sim \mathcal{N}(0, \sigma^2_{S_{B|A}})$
- $S_{B|A}$ provides each participant with their own regression weight: overall $\beta_{B|A}$ plus idiosyncratic $S_{B|A}$.
- As parameter, model estimates variance of random effects vector, $\sigma^2_{S_{B|A}}$.
- As $S_{B|A}$ alters the regression slope $\beta_{B|A}$, also known as *random slope*.
- In `lme4::lmer` syntax: `lmer(if_A_then_B ~ B_given_A*rel_cond + (0+B_given_A|p_id), dat2)`


---
class: small

### A More Reasonable Mixed Model

First model did not allow for different intercepts, $\beta_0$, for each participant. 

$$y=\beta_0 + S_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + \beta_{r}X_{r} + \beta_{I}X_{I} + \epsilon$$
- Model now has a random intercept, $S_0$, and a random slope, $S_{B|A}$.
- $S$-vectors still zero-centered vectors of participant specific random effects. However, we now estimate both, variance and covariance of random effects:
  
$$\left( \begin{matrix} S_0 \\ S_{B|A} \end{matrix} \right)
 \sim \mathcal{N}\left( \left[ \begin{matrix} 0 \\ 0 \end{matrix} \right] , \left[ \begin{matrix} \sigma^2_{S_0}&\rho_{S_{0},S_{B|A}}\sigma_{S_{0}}\sigma_{S_{B|A}} \\ \rho_{S_{B|A},S_{0}}\sigma_{S_{0}}\sigma_{S_{B|A}}&\sigma^2_{S_{B|A}} \end{matrix} \right] \right)$$

Each $S_i$ idiosyncratic intercept & slope:  
`lmer(if_A_then_B ~ B_given_A*rel_cond + (B_given_A|p_id), dat2)`

.pull-left[
```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=3.5, warning=FALSE}
m_tmp <- lmer(if_A_then_B_c ~ B_given_A_c*rel_cond + (1+B_given_A_c|p_id), dat2)
rnd_coefs <- as_tibble(coef(m_tmp)$p_id)
rnd_coefs2 <- bind_rows(
  tibble(
    rel_cond = factor("PO", levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` + rnd_coefs$rel_cond1,
    slope = rnd_coefs$B_given_A_c + rnd_coefs$`B_given_A_c:rel_cond1`
  ),
  tibble(
    rel_cond = factor(c("IR"), levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` - rnd_coefs$rel_cond1,
    slope = rnd_coefs$B_given_A_c - rnd_coefs$`B_given_A_c:rel_cond1`
  )
)

ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs2, aes(intercept = intercept, slope = slope), 
              size = 1.2, color = "lightgrey") +
  geom_abline(data = slopes, aes(intercept = intercept, slope = slope), 
              size = 1.5) +
  facet_wrap(~ rel_cond) + 
  coord_fixed()
```
]

---
class: small
### Maximal By-Participant Mixed Model

$$y=\beta_0 + S_0 + (\beta_{B|A}+ S_{B|A})X_{B|A} + (\beta_{r}+ S_{r})X_{r} + (\beta_{I}+ S_{I})X_{I} + \epsilon$$
- Model estimates 4 variances for zero-centered $S$ vectors, 1 residual variance, and 6 correlations among random effects.

--
.pull-left[
```{r}
library("lme4")
m_p_max <- 
  lmer(if_A_then_B_c ~ B_given_A_c*rel_cond + 
         (B_given_A_c*rel_cond|p_id), dat2)
summary(m_p_max)$varcor
summary(m_p_max)$coefficients

```
]

.pull-right[
```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=4, warning=FALSE}
rnd_coefs <- as_tibble(coef(m_p_max)$p_id)
rnd_coefs2 <- bind_rows(
  tibble(
    rel_cond = factor("PO", levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` + rnd_coefs$rel_cond1,
    slope = rnd_coefs$B_given_A_c + rnd_coefs$`B_given_A_c:rel_cond1`
  ),
  tibble(
    rel_cond = factor(c("IR"), levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` - rnd_coefs$rel_cond1,
    slope = rnd_coefs$B_given_A_c - rnd_coefs$`B_given_A_c:rel_cond1`
  )
)

ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs2, aes(intercept = intercept, slope = slope), 
              size = 1.2, color = "lightgrey") +
  geom_abline(data = slopes, aes(intercept = intercept, slope = slope), 
              size = 1.5) +
  facet_wrap(~ rel_cond) + 
  theme_light() + coord_fixed() +
  theme(text = element_text(size=20))
```
]

---
class: small
## Crossed Random-Effects

So far only considered *participant* as random-effects grouping factor:
- Each participant provides several responses: Random intercept allows idiosyncratic intercept.
- `B_given_A` and `rel_cond` are within-subjects variables: Random slopes allow idiosyncratic effects.

Participant is only one source of stochastic variability. We usually want to generalize over both *participants* and *items*. 
- Example data: Each participant provides 1 response for each of 12 items with condition randomly selected.
- All factors also vary within-items.

Mixed models allow multiple independent random effects (where $I$ are vectors of item-specific offsets):

$$y=\beta_0 + S_0 + I_0 + (\beta_{B|A} + S_{B|A} + I_{B|A})X_{B|A} + (\beta_{r} + S_{r}+ I_{r})X_{r} + (\beta_{I}+ S_{I}+ I_{I})X_{I} + \epsilon$$

```{r}
m_max <- lmer(if_A_then_B_c ~ B_given_A_c*rel_cond + 
                (B_given_A_c*rel_cond|p_id) + 
                (B_given_A_c*rel_cond|i_id), 
              dat2)
```


---
class: small

```{r}
summary(m_max)
```

---
class:small

### Effect of Partial-Pooling / Hierarchical Shrinkage / Regularization

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
library("broom")
no_pooling_estimates <- dat2 %>% 
  group_by(p_id, rel_cond) %>% 
  do(tidy(lm(if_A_then_B_c~B_given_A_c, .))) %>% 
  filter(term == "B_given_A_c") %>% 
  rename(no_pooling = estimate)

partial_pooling_estimates <- data.frame(p_id = rownames(coef(m_max)$p_id),
           PO = coef(m_max)$p_id[,2] + coef(m_max)$p_id[,4],
           IR = coef(m_max)$p_id[,2] - coef(m_max)$p_id[,4])
partial_pooling_estimates <- tidyr::gather(partial_pooling_estimates, key = "rel_cond", value = "partial_pooling", PO, IR)

estimates <- left_join(no_pooling_estimates, partial_pooling_estimates)

```

.pull-left[

Comparison of no-pooling and partial-pooling (i.e., LMM) estimates for `B_given_A` slopes:

```{r, echo=FALSE, out.width='500px', out.height='300px', dpi = 500, fig.width=7, fig.height=7*3/5}

ggplot(data = estimates) + 
  geom_point(mapping = aes(x = no_pooling, y = partial_pooling), alpha = 1.0, pch = 16) + 
  facet_grid(rel_cond ~ .) + 
  coord_fixed() + 
  geom_abline(slope = 1, intercept = 0) + 
  theme(text=element_text(size=18))

```

]


.pull-right[

Distribution of no-pooling and partial-pooling (i.e., LMM) estimates for `B_given_A` slopes:

```{r, echo=FALSE, out.width='400px', out.height='350px', dpi = 500, fig.width=7, fig.height=7*35/40}
estimates_l <- estimates %>% 
  gather("key","estimate",no_pooling, partial_pooling) 

ggplot(data = estimates_l, aes(estimate)) + 
  geom_histogram(binwidth = 0.2) + 
  facet_grid(key ~ rel_cond) +
  theme(text=element_text(size=18))
```
]

- If individual effects can be assumed to come from one normal distribution, partial-pooling provides better estimates than no-pooling even on the individual level (at least on average).
- a.k.a. *James-Stein Estimation* (e.g., Efron & Hastie, 2016, ch. 7) or *Stein's phenomenon*, more generally *regularization*: *ridge regression*, *lasso*, *penalized least squares*, *penalized likelihood*, ...

---
class:small

```{r, echo=FALSE, out.width='1000px', out.height='500px', dpi = 500, fig.width=10, fig.height=5}

df_gravity <- as.data.frame(summary(emmeans::emtrends(m_fixed, "rel_cond", var = "B_given_A_c")))
df_gravity <- df_gravity %>% 
  select(rel_cond, B_given_A_c.trend) %>% 
  spread(rel_cond, B_given_A_c.trend) %>% 
  mutate(key = "complete_pooling")

estimates_l %>% 
  select(-std.error, -statistic, -p.value) %>% 
  spread(rel_cond, estimate) %>% 
  na.omit() %>% 
  ungroup %>% 
  ggplot() + 
  aes(x = PO, y = IR, color = key) + 
  geom_point(size = 2) + 
  geom_point(data = df_gravity, size = 5) + 
  # Draw an arrow connecting the observations between models
  geom_path(aes(group = p_id, color = NULL, alpha = 0.1), 
            arrow = arrow(length = unit(.02, "npc"))) + 
  theme(legend.position = "bottom") + 
  ggtitle("Pooling of regression parameters") + 
  scale_color_brewer(palette = "Dark2") 

```

- from Tristan Mahr: https://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/

---
class: small
### Types of Random Effects

![](random_effect_types.png)
---
class: small, inline-grey

### Crossed Versus Nested


- Factor `A` is **crossed** with factor `B` if multiple levels of `A` appear within multiple levels of `B`. Note that this definition allows for missing values (i.e., it does not need to hold that all levels of `A` appear in all levels of `B`). For example:
  - Levels `a1`, `a2`, ... of `A`  appear in `b1` of `B` and in `b2` of `B`, etc. 
  - A within-subject factor (e.g., `congruency`) is crossed with participants. 
  - If each participant responds to a random subset of items and each item is responded to by several participants, `participant` and `item` are crossed.


- Factor `A` is **nested** within factor `B` if some levels of `A` appear only within specific levels of factor `B`. E.g.:
  - Levels `a1`, `a2`, and `a3` of `A`  appear only in `b1` of `B` and `a4`, `a5`, and `a6` of `A`  appear only in `b2` of `B` ...
  - If student can be member of one class only and several classes were observed, factor `student` is nested within factor `class`.


- Both dependency structures dealt with in same conceptual manner, via independent random effects-parameters. Specifically, both need independent random effects terms in model formula. For example:
  - For `students` nested within `class`, where each student has unique label (i.e., `student` id 1 is assigned to exactly one student and not to different students in different classes), at least:  
    `... + (1|student) + (1|class)`
  - If additional factor `A` is crossed with `class`, but not with `student` (e.g., some students in each class receive treatment `a1`,  some others `a2`), by-class random slopes need to be added:  
    `... + (1|student) + (A|class)`

---

### Interim Summary II

*Fixed-effects parameters*: Overall (or population-average) effect of specific model term on dependent variable

*Random-effects parameters*: 
- zero-centered offsets/displacements for each level of random-effects grouping factor
- added to specific fixed-effects parameter (can also be added in absence of corresponding fixed-effects parameter)
- called *random-intercept* if added to the intercept, and *random-slope* if added to any other parameter.
- assumed to follow normal distribution which provides hierarchical shrinkage, thereby avoiding overfitting
- should be added to each parameter that varies within the levels of a random-effects grouping factor (i.e., factor is *crossed* with random-effects grouping factor)
- Note: random-slopes can only be added if there exist multiple data points (i.e., replications) for each level of random-effects grouping factor and each parameter (e.g., each cell of corresponding factor or design-cell) (e.g., http://singmann.org/mixed-models-for-anova-designs-with-one-observation-per-unit-of-observation-and-cell-of-the-design/)
- mixed model can have random-effects for multiple random-effects grouping factor (e.g., factors one wants to generalize over)
- whether data has only a single random-effects grouping factor, crossed random-effects grouping factors, or nested random-effects grouping factors, random-effects parameters is only mechanism for handling dependencies in mixed models

---

class: center, middle, inverse

# Exercise 2 PP: Pooling Revisited

(see `exercises\exercise2_PP.Rmd` in folder or handout)

---
class: center, middle, inverse

# Practical Issues with Mixed Models in `R`


---
class: small

# Hypothesis-Tests for Mixed Models

`lme4::lmer` does not include *p*-values.

`afex::mixed` provides four different methods:
1. Kenward-Roger (`method="KR"`, default): Provides best-protection against anti-conservative results, requires a lot of RAM for complicated random-effects structures.
2. Satterthwaite (`method="S"`): Similar to KR, but requires less RAM.
3. Parametric-bootstrap (`method="PB"`): Simulation-based, can take a lot of time (can be speed-up using parallel computation).
4. Likelihood-ratio tests (`method="LRT"`): Provides worst control for anti-conservative results. Can be used if all else fails or if all random effects grouping factors have large numbers of levels (e.g., over 50).

Methods do not differ for example data:

```{r, eval=FALSE}
library("afex")
mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond|p_id), dat2, dat2, method = "KR") 
mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond|p_id), dat2, dat2, method = "S") 
mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond|p_id), dat2, method = "LRT") 
# mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond|p_id), dat2, method = "PB") 
```

---
class:small

# Specifying Random-Effects Structure

- Omitting random-effects parameters for model terms which vary within the levels of a random-effects grouping factor and for which random variability exists leads to non-iid residuals (i.e., $\epsilon$) and potentially anti-conservative results (e.g., Barr, Levy, Scheepers, & Tily, 2013).
- Safeguard is **maximal model justified by the design** (Barr et al., 2013).
    - Which factors/terms vary within levels of (i.e. are crossed with) each random-effects grouping factor? 
    - Are there replicates within factor levels (or parameters/coefficients) for levels of random-effects grouping factor?
--


- If maximal model is overparameterized and/or contains degenerate estimates or singular fits (can be indicated by convergence warnings/messages), power of maximal model can be reduced and a reduced model should be used (Bates et al., 2015; Matuschek et al., 2017).
  - Start by removing correlation among random-effects parameters
  - Remove random-effects parameters with variance of 0 and/or for highest-order effects with lowest variance
  - It can sometimes help to try different optimizers (in `afex::mixed` `all_fit = TRUE`)
  - Compare *p*-values/fixed-effects estimates across models (*p*-values from degenerate/minimal models are not reliable)
  - **Warning:** Reducing model introduces unknown risk of anti-conservativity, and should be done with caution!


---

### High-Level Overview

Steps for running a mixed model analysis:
1. Identify desired fixed-effects structure
2. Identify random-effects grouping factors
3. Identify *maximal random-effects structure justified by the design*:
    - Which factors/terms vary within levels of (i.e. are crossed with) each random-effects grouping factor? 
    - Are there replicates within factor levels (or parameters/coefficients) for levels of random-effects grouping factor?
4. Choose method for calculating *p*-values and fit maximal model
5. In case of singular fits: Iteratively reduce random-effects structure until all degenerate random-effects parameters are removed/fit is not singular.

---
class: small

### Suppressing Correlations Among Random Effects

```{r, echo=FALSE, results='hide', message=FALSE}
library("afex")
```

.pull-left[

- `lmer` allows suppressing correlation among random effects using `||` syntax. However, **only for numerical variables.**
- `afex::mixed()` allows using `||` also for factors if `expand_re = TRUE`:

```{r, results='hide', message=FALSE}
m_red <- mixed(
  if_A_then_B_c ~ B_given_A_c*rel_cond + 
    (B_given_A_c*rel_cond||p_id), 
  dat2, method = "S", 
  expand_re = TRUE)
```
```{r}
summary(m_red)$varcor
```
]

.pull-right[
```{r}
m_red
```

```{r, echo=FALSE, dpi=500, fig.width=7, fig.height=4, warning=FALSE}
rnd_coefs <- as_tibble(coef(m_red$full_model)$p_id)
rnd_coefs2 <- bind_rows(
  tibble(
    rel_cond = factor("PO", levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` + 
      rnd_coefs$rel_cond1 + rnd_coefs$re1.rel_cond1,
    slope = rnd_coefs$B_given_A_c + rnd_coefs$`B_given_A_c:rel_cond1` + rnd_coefs$re1.B_given_A_c + rnd_coefs$re1.B_given_A_c_by_rel_cond1
  ),
  tibble(
    rel_cond = factor(c("IR"), levels = c("PO", "IR")),
    intercept = rnd_coefs$`(Intercept)` - 
      (rnd_coefs$rel_cond1 + rnd_coefs$re1.rel_cond1),
    slope = rnd_coefs$B_given_A_c + rnd_coefs$re1.B_given_A_c - (rnd_coefs$`B_given_A_c:rel_cond1` + rnd_coefs$re1.B_given_A_c_by_rel_cond1)
  )
)

ggplot(dat2, aes(y = if_A_then_B_c, x = B_given_A_c)) +
  geom_point(alpha = 0.2, pch = 16, size = 3) + 
  geom_abline(data = rnd_coefs2, aes(intercept = intercept, slope = slope), 
              size = 1.2, color = "lightgrey") +
  geom_abline(data = slopes, aes(intercept = intercept, slope = slope), 
              size = 1.5) +
  facet_wrap(~ rel_cond) + 
  coord_fixed()
```
]

---

## Using `afex` I

```{r}
library("afex")
ma_1 <- mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond|p_id) + 
                (B_given_A_c*rel_cond|i_id), dat2, method = "S") 

ma_2 <- mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond||p_id) + 
                (B_given_A_c*rel_cond||i_id), dat2, method = "S", expand_re = TRUE) 
```

---

## Using `afex` II

```{r}
summary(ma_2)$varcor

ma_3 <- mixed(if_A_then_B_c ~ B_given_A_c*rel_cond + (B_given_A_c*rel_cond||p_id) + 
                (B_given_A_c+rel_cond||i_id), dat2, method = "S", expand_re = TRUE) 
```

---


## Using `afex`: ANOVA Table

```{r}
ma_3 ## or: nice(ma_2)
```

- Usually one is interested in tests of fixed effects. `afex` `print()` (or `nice()`) method presents convenient ANOVA table.

---
class: small

```{r}
summary(ma_3) ## lme4 summary() output
```

---
class: small

```{r}
summary(ma_3)$varcor
```

--

```{r}
summary(ma_3)$coefficients %>% zapsmall
```


---
class: small

## Skovgaard-Olsen et al. (2016): Follow-up Analyses
.pull-left[

```{r, message=FALSE}
nice(ma_3) %>% as.data.frame()
```

]

.pull-right[

```{r}
library("emmeans")
emm_options(lmer.df = "asymptotic") 
# or "Kenward-Roger" or "Satterthwaite"
emmeans(ma_3, "rel_cond")
```

]

---
class: small

## Using `afex_plot`

.pull-left[
```{r}
p1 <- afex_plot(ma_3, "rel_cond")

p2 <- afex_plot(ma_3, "rel_cond", 
                random = "p_id", 
                data_geom = ggpol::geom_boxjitter,
                mapping = "fill")
```

]

.pull-right[
```{r, fig.width=5.5, fig.height=4, dev='svg'}
p2
```

]

---
class: small

### Mixed Models in R: Follow-up Analyses

```{r}
emm_options(lmer.df = "asymptotic") 
# or "Kenward-Roger" or "Satterthwaite"
emtrends(ma_3, "rel_cond", var = "B_given_A_c")
```

```{r}
fixef(ma_3$full_model)[2] + fixef(ma_3$full_model)[4] 
```


More examples of interplay between `mixed` or `lmer` with `emmeans`:
- https://cran.r-project.org/package=afex/vignettes/afex_mixed_example.html
- https://cran.r-project.org/package=emmeans/vignettes/sophisticated.html#lmer

---

class: center, middle, inverse

# Exercise 2 MRE: Maximal Random-Effects Structure

(see `exercises\exercise2_MRE.Rmd` in folder or handout)

---
class: small, inline-grey

# Freeman, Heathcote, Chalmers, and Hockley (2010): Maximal Model

- `task`: between-participants, but within-items (i.e., nested within item)
- `stimulus`: within-participants (i.e., nested within participants), but between items (i.e., each item is either word or non-word)
- `density`: within-participants, but between items
- `frequency`: within-participants, but between items
- `length`: within-participants, but between items

```{r, eval=FALSE}
m_fhch <- mixed(log_rt ~ task*stimulus*density*frequency*length + 
                  (stimulus*density*frequency*length||id) +
                  (task||item), fhch2010, 
                method = "S", expand_re = TRUE)
```


---
class: small

### Example Data: Productivity Scores for Machines and Workers

```{r}
data("Machines", package = "MEMSS")
```

- `Worker`: `Factor` giving unique identifier for the worker.
- `Machine`: `Factor` with levels A, B, and C identifying machine brand.
- `score`: Overall productivity score taking into account number and quality of components produced.

Research question: Do the machines differ in their score?

```{r, include=FALSE}
library("tidyverse")
```

.pull-left[

```{r, fig.height=4, dev='svg', echo=FALSE}
ggplot(Machines, aes(x = Machine, y = score)) +
  geom_point() + 
  facet_wrap(~ Worker) + 
  theme_light()
```

]

--

.pull-right[
Fixed-effects model (without aggregation)

```{r}
mach1 <- lm(score ~ Machine, Machines)
car::Anova(mach1, type = 3)
```


]

---

### Example Data: Productivity Scores for Machines and Workers

```{r}
data("Machines", package = "MEMSS")
```

- `Worker`: `Factor` giving unique identifier for the worker.
- `Machine`: `Factor` with levels A, B, and C identifying machine brand.
- `score`: Overall productivity score taking into account number and quality of components produced.

Research question: Do the machines differ in their score?

```{r, include=FALSE}
library("tidyverse")
```

.pull-left[

Fixed-effects model (without aggregation)

```{r}
mach1 <- lm(score ~ Machine, Machines)
car::Anova(mach1, type = 3)
```

]


.pull-right[

Mixed-model

```{r, results="hide"}
(mach2 <- mixed(score~Machine+
                (Machine|Worker), Machines))
```

```{r, echo=FALSE}
mach2 
```


]



---

### Mixed Models in R: Follow-up Analyses

.pull-left[
**Fixed-effects model**

```{r}
pairs(emmeans(mach1, "Machine"),
      adjust = "holm")
```

]

.pull-right[

**Mixed-effects model**

```{r}
emm_options(lmer.df = "Satterthwaite")
## or "Kenward-Roger"
pairs(emmeans(mach2, "Machine"),
      adjust = "holm")
```

]

---
class: center, middle, inverse

# Many Labs 2
# (Last Exercise: `exercise2_ML2.Rmd` or handout)

---
class: small

 

> **Many Labs 2 abstract:** We conducted preregistered replications of 28 classic and contemporary published findings with protocols that were peer reviewed in advance to examine variation in effect magnitudes across sample and setting. Each protocol was administered to approximately half of 125 samples and 15,305 total participants from 36 countries and territories. Using conventional statistical significance (p < .05), fifteen (54%) of the replications provided evidence in the same direction and statistically significant as the original finding. With a strict significance criterion (p < .0001), fourteen (50%) provide such evidence reflecting the extremely high powered design. Seven (25%) of the replications had effect sizes larger than the original finding and 21 (75%) had effect sizes smaller than the original finding. 

> The median comparable Cohen's d effect sizes for original findings was 0.60 and for replications was 0.15. Sixteen replications (57%) had small effect sizes (< .20) and 9 (32%) were in the opposite direction from the original finding. 


> Across settings, 11 (39%) showed significant heterogeneity using the Q statistic and most of those were among the findings eliciting the largest overall effect sizes; only one effect that was near zero in the aggregate showed significant heterogeneity. Only one effect showed a Tau > 0.20 indicating moderate heterogeneity. Nine others had a Tau near or slightly above 0.10 indicating slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. 

> Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.

For paper and materials see: https://osf.io/8cd4r/

---
class: small

- Goal: Reanalyze data from 3 samples and 1 task, syllogisms, used in Many Labs 2.
- Between-subjects manipulation supposedly affected fluency of syllogisms: Hard to read vs. easy to read font.
- Hypothesis: Decreased fluency makes individuals think harder and thus they solve more syllogisms.

Syllogisms are logical arguments with two premises. For example:

> All dentists are golfers.  
> Some dentists are not tennis players.  
> What, if anything, follows

Participants have to select the correct answer (what follows with logical necessity assuming the premises listed above are true) among a set of 9 possible conclusions listed below:

1. All golfers are tennis players.
2. No tennis players are golfers.
3. All tennis players are golfers.
4. No golfers are tennis players.
5. Some golfers are not tennis players.
6. Some golfers are tennis players.
7. Some tennis players are not golfers.
8. Some tennis players are golfers.
1. Cannot reach a conclusion.

- Main tasks: Identify maximal random-effects structure, fit data with mixed model, evaluate whether hypothesized pattern was observed. For more see exercise sheet.

---
class: center, middle, inverse

# Additional Topics



---
class: small

```{r, echo=FALSE, message=FALSE, results='hide'}
library("sjstats")
```

.pull-left[
### Intraclass Correlation Coefficient (ICC)

Assumption of mixed models: Data points from same unit of observation more similar to each other than unrelated data point.

**Intraclass correlation coefficient (ICC)**: Measure of similarity for data points of a given random-effects grouping factor ranging from 0 to 1:
$$\rho=\frac{\sigma^2_S}{\sigma^2_S+\sigma^2_\epsilon}$$

> The intraclass correlation $\rho$ can also be interpreted as the expected correlation between two randomly drawn units that are in the same group. (Hox, 2010, p. 15)
]

.pull-right[
```{r}
m1 <- lmer(if_A_then_B_c ~ 1 + (1|p_id), dat2)
# summary(m1)
# Random effects:
#  Groups   Name        Variance Std.Dev.
#  p_id     (Intercept) 0.00572  0.0757  
#  Residual             0.14607  0.3822  
# Number of obs: 752, groups:  p_id, 94

0.00572 / (0.0057+0.1461)
library("sjstats")
icc(m1)
```
]
---
class: small

### Intraclass Correlation Coefficient (ICC)

.pull-left[
```{r}
m1 <- lmer(if_A_then_B_c ~ 1 + (1|p_id), dat2)
# summary(m1)
# Random effects:
#  Groups   Name        Variance Std.Dev.
#  p_id     (Intercept) 0.00572  0.0757  
#  Residual             0.14607  0.3822  
# Number of obs: 752, groups:  p_id, 94

icc(m1)
```
]

.pull-right[
```{r, warning=FALSE, message=FALSE}
m2 <- lmer(if_A_then_B_c ~ 1 + 
             (rel_cond:B_given_A_c|p_id), dat2)
# summary(m2)
 # Groups   Name                   Variance Std.Dev. Corr       
 # p_id     (Intercept)            0.0398   0.200               
 #          rel_condPO:B_given_A_c 1.0186   1.009    -0.94      
 #          rel_condIR:B_given_A_c 0.3262   0.571    -0.48  0.75
 # Residual                        0.0570   0.239               
icc(m2)
## Caution! ICC for random-slope-intercept models usually 
## not meaningful. See 'Note' in `?icc`.
```
]

- ICC can be useful to determine if mixed models necessary or to determine how multilevel-clustering effects response.
- ICC often not very useful for random-slope models (Goldstein et al., 2002), direct inspection of SDs often more helpful. 

---
class: inline-grey, small
.pull-left[
## LMMs: Residuals

```{r, fig.width=5, fig.height=5}

dat2$residuals <- 
  residuals(ma_3$full_model)
ggplot(dat2, aes(sample = residuals)) +
  stat_qq() + 
  stat_qq_line() +
  theme_bw() +
  theme(text=element_text(size=18))
```
]


.pull-right[
- We can inspect normality assumption of LMM using QQ-plots via `residuals()`
- For `afex::mixed` make sure to pass `full_model` slot to `residuals()`.
- Alternatively, use `qqmath.merMod()` (see `?qqmath.merMod`).
- Here, very heavy tails in residuals  (i.e., leptokurtic) and no transformation can help. Without more ambitious model we have to settle.
    - We know normality assumption must be violated for data on restricted scale 
    - We are not interested in coefficients and prediction 
    - Inference agrees with visual impression


```{r, fig.width=5, fig.height=2.5, echo=FALSE}
dat2 %>% 
  select(if_A_then_B, B_given_A) %>% 
  gather(key, value, if_A_then_B, B_given_A) %>% 
  mutate(key = factor(key, levels = c("if_A_then_B", "B_given_A"))) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 101) +
  facet_wrap(~key) +
  theme_bw() +
  theme(text=element_text(size=18))
```

]

---
class: inline-grey

# GLMMs: Mixed-models with Alternative Distributional Assumptions

- Not all data can be reasonable described by a Normal distribution.
- Generalized-linear mixed models (GLMMs; e.g., Jaeger, 2008) allow for other distributions: 
    - Binomial distribution: Repeated-measures logistic regression
    - Poisson distribution for count data
    - Gamma distribution for non-negative data (e.g., RTs)
    - ...
- GLMMs require specification of residual distribution (`family`) and link function.
- Link function determines how values on untransformed scale are mapped onto response scale.
- Specification of random-effects structure conceptually identical as for LMMs.
- GLMMs only allow two methods for hypothesis testing: `"LRT"` or `"PB"`.
- Inspection of residuals/model fit more important for GLMMs than for LMMs: R package [`DHARMa`](https://cran.r-project.org/package=DHARMa)
- Can be fit with `lme4::glmer` or `afex::mixed`, both require `family` argument (e.g., `family = binomial`).

---
class: small

### GLMM Example

```{r, eval=FALSE}
data("fhch2010")
fhch2 <- droplevels(fhch2010[fhch2010$task == "lexdec", ] )
gm1 <- mixed(correct ~ stimulus + (stimulus||id) + (stimulus||item), 
             fhch2, family = binomial,      # implies: binomial(link = "logit")
             method = "LRT", expand_re = TRUE) # alt: binomial(link = "probit")
gm1  
## Mixed Model Anova Table (Type 3 tests, LRT-method)
## 
## Model: correct ~ stimulus + (stimulus || id) + (stimulus | item)
## Data: fhch2
## Df full model: 7
##     Effect df Chisq p.value
## 1 stimulus  1  1.19     .28
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
## Warning messages: [...]
emmeans(gm1, "stimulus", type = "response")
##  stimulus   prob       SE df asymp.LCL asymp.UCL
##  word     0.9907 0.002323 NA    0.9849    0.9943
##  nonword  0.9857 0.003351 NA    0.9774    0.9909
## 
## Confidence level used: 0.95 
## Intervals are back-transformed from the logit scale 
```

- Removing correlations does not remove warnings. Likely reason: Data at boundary.
- Alternative specification for Binomial model: DV is proportion + `weight` (= N observations) 

---

### Summary 

- Mixed models allow accounting for dependencies originating from clustered data via random effects.
- Fixed effects represent the overall/average effect; random effects add offset to specific fixed effects.
- Random-effects parameters are variances of offsets and covariances among different random-effects parameters.
- Random-effects assumed to follow normal distribution thereby implementing partial-pooling / hierarchical-shrinkage (regularization).
- Mixed-model analysis should start with maximal model which includes random slopes for all effects that vary within a random-effects grouping factor.
- If maximal model fails to converge or is singular/degenerate: Start by removing correlation, then highest-order effects.


- Linear mixed models (LMMs) can be extended to generalized linear mixed models (GLMMs) that support different link function and residual distribution (via `glmer` or `mixed`).
- Bayesian estimation possible: `rstanarm::stan_lmer`, `blme`, `MCMCglmm`, or `brms`.
- More powerful: generalized additive mixed models (GAMM; Baayen, Vasisth, Kliegl, & Bates, 2017).


- An overview of these topics is also provided in: 
  Singmann and Kellen (in press). An Introduction to Mixed Models for Experimental Psychology. In D. H. Spieler & E. Schumacher (Eds.), *New Methods in Neuroscience and Cognitive Psychology*. Psychology Press. http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf

---

class: small
### References Mixed Modeling:
- Singmann, H., & Kellen, D. (in press). An Introduction to Mixed Models for Experimental Psychology. In D. H. Spieler & E. Schumacher (Eds.), *New Methods in Neuroscience and Cognitive Psychology*. Psychology Press. http://singmann.org/download/publications/singmann_kellen-introduction-mixed-models.pdf
- Baayen, H., Vasishth, S., Kliegl, R., & Bates, D. (2017). The cave of shadows: Addressing the human factor with generalized additive mixed models. *Journal of Memory and Language*, 94, 206-234. https://doi.org/10.1016/j.jml.2016.11.006
- Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. *Journal of Memory and Language*, 68(3), 255-278. https://doi.org/10.1016/j.jml.2012.11.001
- Bates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). *Parsimonious Mixed Models.* arXiv:1506.04967 [stat]. http://arxiv.org/abs/1506.04967
- Goldstein, H., Browne, W., & Rasbash, J. (2002). Partitioning Variation in Multilevel Models. *Understanding Statistics*, 1(4), 223-231. https://doi.org/10.1207/S15328031US0104_02
- Hox, J. J. (2010). *Multilevel analysis: techniques and applications.* New York: Routledge.
- Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. _Journal of Memory and Language_, 59(4), 434-446. https://doi.org/10.1016/j.jml.2007.11.007
- Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing Type I error and power in linear mixed models. *Journal of Memory and Language*, 94, 305-315. https://doi.org/10.1016/j.jml.2017.01.001
- https://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/
- https://cran.r-project.org/package=DHARMa


---
class: small

### References Example Data:
- Skovgaard-Olsen, N., Singmann, H., & Klauer, K. C. (2016). The relevance effect and conditionals. *Cognition*, 150, 26-36. https://doi.org/10.1016/j.cognition.2015.12.017

---
class: small

### Residuals

.pull-left[

```{r, fig.width=5, fig.height=4}
plot(m_max, 
     resid(.,scaled=TRUE) ~ B_given_A | rel_cond)
```
]

.pull-right[
```{r, fig.width=4, fig.height=4}
lattice::qqmath(m_max)
```

]

```{r, eval=FALSE}
plot(m_max, p_id ~ resid(., scaled=TRUE) )
plot(m_max, resid(., scaled=TRUE) ~ fitted(.) | rel_cond)
?plot.merMod
```

- https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf
- https://htmlpreview.github.io/?https://github.com/bbolker/iiscvisit/blob/master/workshops/mixedlab.html

---

### Interpreting Residuals
- Zuur et al. (2009). *Mixed Effects Models and Extensions in Ecology with R.* Springer. [Chapter 2]
- Farraway (2002). *Practical Regression and Anova using R*. https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf [Chapter 7]

### Further Diagnostics
- http://janhove.github.io/analysis/2018/04/25/graphical-model-checking
- https://cran.r-project.org/web/packages/influence.ME/index.html
- https://cran.r-project.org/web/packages/DHARMa/index.html

```{r, eval=FALSE, include=FALSE}
library("afex")
load("ssk16_dat_tutorial.rda")
```

```{r, eval=FALSE, include=FALSE}

m_full <- mixed(if_A_then_B_c ~ B_given_A_c*rel_cond +
                       (rel_cond*B_given_A_c|p_id) +
                       (rel_cond*B_given_A_c|i_id),
                     dat,
                     control = lmerControl(optCtrl = list(maxfun=1e8)),
                     method = "S")

save(m_full, file = "fitted_lmms.rda", compress = "xz")
```
